{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "from operator import add"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BanditGameNonStationary:\n",
    "    def __init__(self, n, mu, sigma):\n",
    "        self.n = n\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.q = np.random.normal(mu, sigma, n)\n",
    "    \n",
    "    def reward(self, a):\n",
    "        rew = self.q[a] + np.random.normal(self.mu, self.sigma, 1)[0]\n",
    "        self.walk()\n",
    "        return rew\n",
    "\n",
    "    def walk(self):\n",
    "        self.q = self.q + np.random.normal(0, 0.01, self.n)\n",
    "\n",
    "    def trueReward(self, a):\n",
    "        return self.q[a]\n",
    "\n",
    "    def bestAction(self):\n",
    "        return np.argmax(self.q)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BanditGame:\n",
    "    def __init__(self, n, mu, sigma):\n",
    "        self.n = n\n",
    "        self.mu = mu\n",
    "        self.sigma = sigma\n",
    "        self.q = np.random.normal(mu, sigma, n)\n",
    "        self.bestA = np.argmax(self.q)\n",
    "\n",
    "    def reward(self, a):\n",
    "        return self.q[a] + np.random.normal(self.mu, self.sigma/2, 1)[0]\n",
    "\n",
    "    def trueReward(self, a):\n",
    "        return self.q[a]\n",
    "\n",
    "    def bestAction(self):\n",
    "        return self.bestA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Player:\n",
    "    def __init__(self, n, eps):\n",
    "        self.eps = eps\n",
    "        self.n = n\n",
    "        self.N = [0 for i in range(n)]\n",
    "        self.Q = [0 for i in range(n)]\n",
    "\n",
    "    def play(self):\n",
    "        return self.Q.index(max(self.Q)) if np.random.random() < (1 - self.eps) else np.random.randint(0, self.n-1)\n",
    "\n",
    "    def update(self, a, R):\n",
    "        self.N[a] = self.N[a] + 1\n",
    "        self.Q[a] = self.Q[a] + (R - self.Q[a])/self.N[a]\n",
    "\n",
    "    def reset(self):\n",
    "        self.N = [0 for i in range(self.n)]\n",
    "        self.Q = [0 for i in range(self.n)]\n",
    "    \n",
    "    def whoIam(self):\n",
    "        return 'Player, eps={}'.format(self.eps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "class UCBPlayer:\n",
    "    def __init__(self, n, c):\n",
    "        self.c = c\n",
    "        self.n = n\n",
    "        self.N = [0 for i in range(n)]\n",
    "        self.Q = [0 for i in range(n)]\n",
    "        self.t = 1\n",
    "\n",
    "    def play(self):\n",
    "        uQ = [qt + self.c*math.sqrt(math.log(self.t)/nt) if nt != 0 else 1e6 for qt, nt in zip(self.Q, self.N)]\n",
    "        return uQ.index(max(uQ))\n",
    "\n",
    "    def update(self, a, R):\n",
    "        self.t = self.t + 1\n",
    "        self.N[a] = self.N[a] + 1\n",
    "        self.Q[a] = self.Q[a] + (R - self.Q[a])/self.N[a]\n",
    "\n",
    "    def reset(self):\n",
    "        self.t = 1\n",
    "        self.N = [0 for i in range(self.n)]\n",
    "        self.Q = [0 for i in range(self.n)]\n",
    "    \n",
    "    def whoIam(self):\n",
    "        return 'UCBPlayer, c={}'.format(self.c)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaptivePlayer:\n",
    "    def __init__(self, n, eps, nswitch):\n",
    "        self.eps = eps\n",
    "        self.epsBase = eps\n",
    "        self.n = n\n",
    "        self.N = [0 for i in range(n)]\n",
    "        self.Q = [0 for i in range(n)]\n",
    "        self.nswitch = nswitch\n",
    "        self.iter = 0\n",
    "        self.h = (self.epsBase - self.epsBase/10) / self.nswitch\n",
    "\n",
    "    def play(self):\n",
    "        if self.iter < self.nswitch:\n",
    "            self.eps = self.epsBase - self.h * self.iter\n",
    "        self.iter = self.iter + 1\n",
    "        return self.Q.index(max(self.Q)) if np.random.random() < (1 - self.eps) else np.random.randint(0, self.n-1)\n",
    "\n",
    "    def update(self, a, R):\n",
    "        self.N[a] = self.N[a] + 1\n",
    "        self.Q[a] = self.Q[a] + (R - self.Q[a])/self.N[a]\n",
    "\n",
    "    def reset(self):\n",
    "        self.N = [0 for i in range(self.n)]\n",
    "        self.Q = [0 for i in range(self.n)]\n",
    "        self.iter = 0 \n",
    "        self.eps = self.epsBase\n",
    "\n",
    "    def whoIam(self):\n",
    "        return 'AdaptPlayer, eps={}, sw={}'.format(self.epsBase, self.nswitch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimisticTrackingPlayer:\n",
    "    def __init__(self, n, eps, alpha, QInit):\n",
    "        self.eps = eps\n",
    "        self.n = n\n",
    "        self.alpha = alpha\n",
    "        self.Q = [QInit for i in range(n)]\n",
    "        self.Qinit = QInit\n",
    "\n",
    "    def play(self):\n",
    "        return self.Q.index(max(self.Q)) if np.random.random() < (1 - self.eps) else np.random.randint(0, self.n-1)\n",
    "\n",
    "    def update(self, a, R):\n",
    "        self.Q[a] = self.Q[a] + self.alpha*(R - self.Q[a])\n",
    "\n",
    "    def reset(self):\n",
    "        self.Q = [self.Qinit for i in range(self.n)]\n",
    "    \n",
    "    def whoIam(self):\n",
    "        return 'OptimisticTrackingPlayer, eps={}, alpha = {}, Qinit={}'.format(self.eps, self.alpha, self.Qinit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OptimisticPlayer:\n",
    "    def __init__(self, n, eps, QInit):\n",
    "        self.eps = eps\n",
    "        self.n = n\n",
    "        self.N = [0 for i in range(n)]\n",
    "        self.Q = [QInit for i in range(n)]\n",
    "        self.Qinit = QInit\n",
    "\n",
    "    def play(self):\n",
    "        return self.Q.index(max(self.Q)) if np.random.random() < (1 - self.eps) else np.random.randint(0, self.n-1)\n",
    "\n",
    "    def update(self, a, R):\n",
    "        self.N[a] = self.N[a] + 1\n",
    "        self.Q[a] = self.Q[a] + (R - self.Q[a])/self.N[a]\n",
    "\n",
    "    def reset(self):\n",
    "        self.N = [0 for i in range(self.n)]\n",
    "        self.Q = [self.Qinit for i in range(self.n)]\n",
    "    \n",
    "    def whoIam(self):\n",
    "        return 'OptimisticPlayer, eps={}, Qinit={}'.format(self.eps, self.Qinit)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrackingPlayer:\n",
    "    def __init__(self, n, eps, alpha):\n",
    "        self.eps = eps\n",
    "        self.n = n\n",
    "        self.alpha = alpha\n",
    "        self.Q = [0 for i in range(n)]\n",
    "\n",
    "    def play(self):\n",
    "        return self.Q.index(max(self.Q)) if np.random.random() < (1 - self.eps) else np.random.randint(0, self.n-1)\n",
    "\n",
    "    def update(self, a, R):\n",
    "        self.Q[a] = self.Q[a] + self.alpha*(R - self.Q[a])\n",
    "\n",
    "    def reset(self):\n",
    "        self.Q = [0 for i in range(self.n)]\n",
    "    \n",
    "    def whoIam(self):\n",
    "        return 'TrackingPlayer, eps={}, a={}'.format(self.eps, self.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TrackingUnbiasedPlayer:\n",
    "    def __init__(self, n, eps, alpha):\n",
    "        self.eps = eps\n",
    "        self.n = n\n",
    "        self.alpha = alpha\n",
    "        self.Q = [0 for i in range(n)]\n",
    "        self.o = 0\n",
    "\n",
    "    def play(self):\n",
    "        return self.Q.index(max(self.Q)) if np.random.random() < (1 - self.eps) else np.random.randint(0, self.n-1)\n",
    "\n",
    "    def update(self, a, R):\n",
    "        self.o = self.o + self.alpha*(1- self.o)\n",
    "        self.Q[a] = self.Q[a] + self.alpha/self.o*(R - self.Q[a])\n",
    "\n",
    "    def reset(self):\n",
    "        self.Q = [0 for i in range(self.n)]\n",
    "        self.o = 0\n",
    "    \n",
    "    def whoIam(self):\n",
    "        return 'TrackingUnbiasedPlayer, eps={}, a={}'.format(self.eps, self.alpha)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TestSuite:\n",
    "    def __init__(self, Bandit, nBandits, n, mu, sigma):\n",
    "        self.nBandits = nBandits\n",
    "        self.bandits = [Bandit(n, mu, sigma) for i in range(nBandits)]\n",
    "\n",
    "    def playBandit(self, player, iBandit, iter):\n",
    "        playerReward = []\n",
    "        isOptimalA = []\n",
    "        for i in range(iter):\n",
    "            a = player.play()\n",
    "            isOptimalA.append(a == self.bandits[iBandit].bestAction())\n",
    "            reward = self.bandits[iBandit].reward(a)\n",
    "            playerReward.append(reward)\n",
    "            player.update(a, reward)\n",
    "\n",
    "        return playerReward, isOptimalA\n",
    "\n",
    "    def playAllBandits(self, player, iter):\n",
    "        avgReward = [0 for i in range(iter)]\n",
    "        fractionOptimalA = [0 for i in range(iter)]\n",
    "        for i in range(self.nBandits):\n",
    "            reward, isOptimalA = self.playBandit(player, i, iter)\n",
    "            avgReward = list( map(add, avgReward, reward) )\n",
    "            fractionOptimalA = list( map(add, fractionOptimalA, isOptimalA) )\n",
    "            player.reset()\n",
    "\n",
    "        avgReward = [e / self.nBandits for e in avgReward]\n",
    "        fractionOptimalA = [e / self.nBandits for e in fractionOptimalA]\n",
    "        return avgReward, fractionOptimalA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "#suite = TestSuite(BanditGameNonStationary, 2000, 10, 0, 1)\n",
    "#players = [TrackingUnbiasedPlayer(10, 0.1, 0.1), TrackingPlayer(10, 0.1, 0.1)]\n",
    "suite = TestSuite(BanditGame, 2000, 10, 0, 1)\n",
    "players = [Player(10, 0.1), UCBPlayer(10, 2), UCBPlayer(10, 1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "j = 1000\n",
    "\n",
    "rewards = []\n",
    "fractionOptimalA = []\n",
    "for p in players:\n",
    "    rews, frOptA = suite.playAllBandits(p, j)\n",
    "    rewards.append(rews)\n",
    "    fractionOptimalA.append(frOptA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in fractionOptimalA:\n",
    "    plt.plot(r, label = players[fractionOptimalA.index(r)].whoIam())\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for r in rewards:\n",
    "    plt.plot(r, label = players[rewards.index(r)].whoIam())\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.3 64-bit",
   "name": "python373jvsc74a57bd031f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "metadata": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  },
  "orig_nbformat": 2
 },
 "nbformat": 4,
 "nbformat_minor": 2
}